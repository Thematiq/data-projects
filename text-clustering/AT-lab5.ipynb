{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "stable-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from tqdm import trange\n",
    "from scipy.sparse.linalg import norm as sparse_norm\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.linalg import norm as dense_norm\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from numba import jit, generated_jit\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-italic",
   "metadata": {},
   "source": [
    "# Ex. 1 using own functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electronic-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram(text: str, n: int) -> dict:\n",
    "    vec = {}\n",
    "    for i in range(len(text)-n+1):\n",
    "        pos = text[i:i+n]\n",
    "        vec[pos] = vec.get(pos, 0) + 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "external-finish",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab': 2, 'bc': 1, 'cd': 1, 'de': 1, 'ef': 1, 'fa': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'abcdefab'\n",
    "v = to_ngram(t, 2)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "located-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.48 µs ± 36.4 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "to_ngram(t, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "remarkable-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_norm_np(a: dict) -> float:\n",
    "    vec = np.fromiter(a.values(), dtype=float)\n",
    "    return np.sqrt(np.sum(np.square(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.68 µs ± 180 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "euc_norm_np(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "precious-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a: dict, b: dict) -> float:\n",
    "    prod = 0\n",
    "    for index, value in a.items():\n",
    "        if index in b:\n",
    "            prod += value * b[index]\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "automotive-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(a: dict, b: dict) -> float:\n",
    "    return 1 - dot_product(a, b) / (euc_norm_np(a) * euc_norm_np(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "local-destiny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = to_ngram('abcdef', 2)\n",
    "b = to_ngram('bcbcbc', 2)\n",
    "dot_product(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "advised-fitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.2 µs ± 280 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cosine_dist(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secret-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(a: dict) -> dict:\n",
    "    size = euc_norm_np(a)\n",
    "    return {\n",
    "        key: val / size\n",
    "        for key, val in a.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sticky-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_euc_dist(a: dict, b: dict) -> float:\n",
    "    a = normalize_vec(a)\n",
    "    b = normalize_vec(b)\n",
    "    c = {}\n",
    "    for key, val in a.items():\n",
    "        c[key] = c.get(key, 0) + val\n",
    "    for key, val in b.items():\n",
    "        c[key] = c.get(key, 0) - val\n",
    "    return euc_norm_np(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "preceding-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_dist(a: dict, b: dict) -> float:\n",
    "    top = 0\n",
    "    bottom = len(a) + len(b)\n",
    "    for item in a:\n",
    "        if item in b:\n",
    "            top += 2\n",
    "    return 1 - top / bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "integral-joyce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1206210744336147"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_euc_dist(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "front-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_dist(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-carnival",
   "metadata": {},
   "source": [
    "# Using scikit-learn\n",
    "\n",
    "Using CountVectorizer will make clustering easier later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blocked-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(file: str) -> list:\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read().split('\\n')\n",
    "    return list(filter(lambda x: len(x) > 0, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "accredited-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(content: list, n: int, stop_words = None, unit='char'):\n",
    "    if unit == 'char' and stop_words is not None:\n",
    "        content = [' '.join(filter(lambda x: x not in stop_words, member.split(' '))) for member in content]\n",
    "    vectorizer = CountVectorizer(input='content',\n",
    "                                 lowercase=True,\n",
    "                                 stop_words=stop_words,\n",
    "                                 ngram_range=(n, n),\n",
    "                                 analyzer=unit)\n",
    "    return vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "stainless-condition",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_file(file: str, n: int, stop_words = None, unit='char'):\n",
    "    content = get_documents(file)\n",
    "    return vectorize(content, n, stop_words, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mineral-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize_file('lines.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subtle-myrtle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[12,  0,  0,  0,  0,  0,  4,  0,  0,  0,  3,  2,  0,  4, 10,  8,\n",
       "          0,  0,  5,  3,  2,  4,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  6,  2,  1,  1,  3,  0,  0,  1,  4,  0,  2,  3,  1,  4,  4,\n",
       "          3,  0,  4,  5,  4,  2,  0,  1,  0,  2,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[1].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lasting-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_cosine_dist(a, b):\n",
    "    return 1 - ((a.dot(b.T)) / (sparse_norm(a) * sparse_norm(b))).todense().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pleased-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sk_dense_cosine_dist(a, b):\n",
    "    return 1 - (a @ b.T) / (dense_norm(a) * dense_norm(b)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "black-patrick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22656300044299682"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_cosine_dist(vec[1], vec[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cultural-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_euc_dist(a, b):\n",
    "    a_norm = a / sparse_norm(a)\n",
    "    b_norm = b / sparse_norm(b)\n",
    "    return 1 - sparse_norm(a_norm - b_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demanding-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sk_dense_euc_dist(a, b):\n",
    "    a_norm = a / dense_norm(a)\n",
    "    b_norm = b / dense_norm(b)\n",
    "    return dense_norm(a_norm - b_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "yellow-client",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32685365566914515"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_euc_dist(vec[1], vec[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suspected-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sk_dense_dice_dist(a, b):\n",
    "    a_items = a > 0\n",
    "    b_items = b > 0\n",
    "    return 1 - 2 * np.sum((a_items & b_items)) / (np.sum(a_items) + np.sum(b_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subject-attendance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sk_dice_dist(a, b):\n",
    "    return sk_dense_dice_dist(a.todense(), b.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "japanese-unknown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_dice_dist(vec[1], vec[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-celtic",
   "metadata": {},
   "source": [
    "# Ex. 2 Clustering Quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exotic-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pred(filename, pred):\n",
    "    raw = get_documents(filename)\n",
    "    clusters = []\n",
    "    for i in range(np.max(pred)+1):\n",
    "        clusters.append([raw[i] for i in np.where(pred == i)[0]])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "obvious-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_pred(clusters: list, **vec_kwargs):\n",
    "    content = sum(map(len, clusters))\n",
    "    raw = [text for cluster in clusters for text in cluster]\n",
    "    pred = np.zeros(len(raw))\n",
    "    pos = 0\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        pred[pos: pos + len(cluster)] = i\n",
    "        pos += len(cluster)\n",
    "    return vectorize(raw, **vec_kwargs), pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "appointed-eligibility",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def from_file(filename):\n",
    "    clusters = [[]]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n', '')\n",
    "            if line == '##########':\n",
    "                clusters.append([])\n",
    "            elif len(line) > 0:\n",
    "                clusters[-1].append(line)\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ongoing-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = from_file('clusters.txt')\n",
    "c_content, c_pred = transform_to_pred(lists, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adult-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    0.,    0., ..., 3353., 3354., 3355.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "strong-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intracluster_distance(cluster: np.ndarray, metric) -> float:\n",
    "    if len(cluster.shape) != 2:\n",
    "        cluster = cluster.reshape(-1, 1)\n",
    "    return np.max(cdist(cluster, cluster, metric=metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ecological-preview",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def intercluster_distance(cluster_a, cluster_b, metric) -> float:\n",
    "    if len(cluster_a.shape) != 2:\n",
    "        cluster_a = cluster_a.reshape(-1, 1)\n",
    "    if len(cluster_b.shape) != 2:\n",
    "        cluster_b = cluster_b.reshape(-1, 1)\n",
    "    return np.min(cdist(cluster_a, cluster_b, metric=metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "announced-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intracluster_table(items, classes, metric, clusters):\n",
    "    out = np.zeros(clusters)\n",
    "    for i in range(clusters):\n",
    "        out[i] = intracluster_distance(items[classes == i], metric=metric)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ongoing-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intercluster_table(items, classes, metric, clusters):\n",
    "    out = np.zeros((clusters, clusters))\n",
    "    for i in trange(clusters):\n",
    "        for j in range(i):\n",
    "            out[i, j] = intercluster_distance(items[classes == i], items[classes == j], metric)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amino-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index(items, classes, metric) -> float:\n",
    "    bottom = 0\n",
    "    top = inf\n",
    "    clusters = int(np.max(classes))\n",
    "    intra = intracluster_table(items, classes, metric, clusters)\n",
    "    inter = intercluster_table(items, classes, metric, clusters)\n",
    "    for i in range(clusters):\n",
    "        intr = intra[i]\n",
    "        if intr > bottom:\n",
    "            bottom = intr\n",
    "        for j in range(i):\n",
    "            intr = inter[i, j]\n",
    "            if intr < top:\n",
    "                top = intr\n",
    "    return top / bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "defined-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_index(items, classes, metric) -> float:\n",
    "    total = 0\n",
    "    clusters = int(np.max(classes))\n",
    "    intra = intracluster_table(items, classes, metric, clusters)\n",
    "    inter = intercluster_table(items, classes, metric, clusters)\n",
    "    for i in range(clusters):\n",
    "        best = 0\n",
    "        for j in range(clusters):\n",
    "            if i == j:\n",
    "                continue\n",
    "            score = (intra[i] + intra[j]) / inter[max(i, j), min(i, j)]\n",
    "            if score > best:\n",
    "                best = score\n",
    "        total += best\n",
    "    return total / clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-antarctica",
   "metadata": {},
   "source": [
    "# Ex. 3 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "divine-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_counter(filename: str, drop_punct: bool = False) -> Counter:\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    tokenizer = English().tokenizer\n",
    "    tokens = tokenizer(content)\n",
    "    words = [\n",
    "        token.text for token in tokens\n",
    "        if (not drop_punct) or (not token.is_punct)    \n",
    "            ]\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "driven-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_stopwords(filename: str, size: int, drop_punct: bool = False) -> list:\n",
    "    counter = grab_counter(filename, drop_punct)\n",
    "    return [x[0] for x in counter.most_common(size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interested-cocktail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '-', '\\n', '.', ':', ' ', 'LTD', 'CHINA', ')', '(']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_popular_stopwords('lines.txt', 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "collected-vacation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " 'LTD',\n",
       " 'CHINA',\n",
       " 'ROAD',\n",
       " 'POLAND',\n",
       " 'LOGISTICS',\n",
       " 'TEL',\n",
       " 'OF',\n",
       " 'CO.,LTD']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_popular_stopwords('lines.txt', 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "liable-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_least_popular_stopwords(filename: str, size: int, drop_punct: bool = False) -> list:\n",
    "    counter = grab_counter(filename, drop_punct)\n",
    "    return [x[0] for x in counter.most_common()[-size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hawaiian-wyoming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['750',\n",
       " '945',\n",
       " '15789',\n",
       " 'jaromir.witas@zing.com.pl',\n",
       " 'ZIPP',\n",
       " 'SKUTERY',\n",
       " 'SzklanychDomow',\n",
       " 'Sliwice',\n",
       " 'SZKLANYCHDOMOW',\n",
       " 'SLIWICE']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_least_popular_stopwords('lines.txt', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-million",
   "metadata": {},
   "source": [
    "# Ex. 4 & 5 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-essence",
   "metadata": {},
   "source": [
    "### 2 ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "gross-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize_file('lines.txt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "accredited-encoding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6751, 2212)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = vec.toarray()\n",
    "dense.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-registrar",
   "metadata": {},
   "source": [
    "We will use DBSCAN which allows for uknown number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "vulnerable-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.2,\n",
    "    min_samples=1,\n",
    "    metric=sk_dense_cosine_dist,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "crazy-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "driven-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3953/3953 [15:18<00:00,  4.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31986720046479894"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(dense, pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "tropical-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3953/3953 [16:47<00:00,  3.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1388442140947126"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(dense, pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "progressive-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.2,\n",
    "    min_samples=3,\n",
    "    metric=sk_dense_cosine_dist,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "judicial-trailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "attached-ozone",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_to_file(filename: str, content: list, pred: np.ndarray):\n",
    "    clusters = np.max(pred)\n",
    "    with open(filename, 'w') as f:\n",
    "        for cluster in range(clusters):\n",
    "            for line in np.where(pred == cluster)[0]:\n",
    "                f.write(content[line] + '\\n')\n",
    "            f.write('########## \\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "recreational-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file('sol.txt', get_documents('lines.txt'), pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-exposure",
   "metadata": {},
   "source": [
    "## Dunn Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-alfred",
   "metadata": {},
   "source": [
    "In simple words, Dunn Index is ratio between smallest intercluster (between different clusters) distance to largest intracluster distance. So we want to maximize this metric (make clusters smaller and further apart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-focus",
   "metadata": {},
   "source": [
    "Cosine dist, minimum samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "legal-sixth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-c771f96a904c>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"sk_dense_cosine_dist\" failed type inference due to: \u001b[1m\u001b[1mNo implementation of function Function(<built-in function matmul>) found for signature:\n",
      " \n",
      " >>> matmul(array(int64, 1d, C), array(int64, 1d, C))\n",
      " \n",
      "There are 2 candidate implementations:\n",
      "\u001b[1m  - Of which 2 did not match due to:\n",
      "  Overload in function 'MatMul.generic': File: numba/core/typing/npydecl.py: Line 994.\n",
      "    With argument(s): '(array(int64, 1d, C), array(int64, 1d, C))':\u001b[0m\n",
      "\u001b[1m   Rejected as the implementation raised a specific error:\n",
      "     TypingError: \u001b[1m'@' only supported on float and complex arrays\u001b[0m\u001b[0m\n",
      "  raised from /home/thmtt/.local/lib/python3.9/site-packages/numba/core/typing/npydecl.py:942\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of intrinsic-call at <ipython-input-22-c771f96a904c> (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-22-c771f96a904c>\", line 3:\u001b[0m\n",
      "\u001b[1mdef sk_dense_cosine_dist(a, b):\n",
      "\u001b[1m    return 1 - (a @ b.T) / (dense_norm(a) * dense_norm(b)).item()\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "/home/thmtt/.local/lib/python3.9/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"sk_dense_cosine_dist\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-22-c771f96a904c>\", line 2:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef sk_dense_cosine_dist(a, b):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/thmtt/.local/lib/python3.9/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-22-c771f96a904c>\", line 2:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef sk_dense_cosine_dist(a, b):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "100%|██████████| 473/473 [01:25<00:00,  5.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3206443972512965"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(dense, pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-parts",
   "metadata": {},
   "source": [
    "Cosine dist, clusters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aboriginal-detroit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3355/3355 [13:51<00:00,  4.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002554282587932777"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(c_content.todense(), c_pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-fusion",
   "metadata": {},
   "source": [
    "## DB Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-hostel",
   "metadata": {},
   "source": [
    "Simply put, DB index is sum of maximum sum of 2 intracluster distances divided by intercluster distance between them. We want to minimize this value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-chassis",
   "metadata": {},
   "source": [
    "Cosine dist, minimum samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "academic-discount",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 473/473 [01:22<00:00,  5.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6410049991735118"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(dense, pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-notice",
   "metadata": {},
   "source": [
    "Cosine dist, clusters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fantastic-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3355/3355 [17:29<00:00,  3.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.215669407345323"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(c_content.todense(), c_pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-spare",
   "metadata": {},
   "source": [
    "## 3 ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rational-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize_file('lines.txt', 3)\n",
    "data = vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "massive-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.2,\n",
    "    min_samples=3,\n",
    "    metric='cosine',\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-navigation",
   "metadata": {},
   "source": [
    "Kernel died for all tries (total of 4, even with sklearn metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-sampling",
   "metadata": {},
   "source": [
    "# Using stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-arlington",
   "metadata": {},
   "source": [
    "Used stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "faced-melissa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '-',\n",
       " '\\n',\n",
       " '.',\n",
       " ':',\n",
       " ' ',\n",
       " 'LTD',\n",
       " 'CHINA',\n",
       " ')',\n",
       " '(',\n",
       " 'ROAD',\n",
       " '/',\n",
       " 'POLAND',\n",
       " '\"',\n",
       " 'LOGISTICS',\n",
       " 'TEL',\n",
       " 'OF',\n",
       " 'CO.,LTD',\n",
       " 'CO',\n",
       " 'RUSSIA']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_popular_stopwords('lines.txt', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "miniature-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize_file(\n",
    "    'lines.txt', 2,\n",
    "    stop_words=get_popular_stopwords('lines.txt', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dietary-platform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6751, 2212)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_stop = vec.todense()\n",
    "dense_stop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "republican-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.1,\n",
    "    min_samples=1,\n",
    "    metric=sk_dense_cosine_dist,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "standing-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(dense_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "loved-theory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4859/4859 [22:03<00:00,  3.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26661129997598526"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(dense_stop, pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "accessible-emergency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4859/4859 [21:07<00:00,  3.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6524205814650448"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(dense_stop, pred, sk_dense_cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "banner-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = from_file('clusters.txt')\n",
    "c_content, c_pred = transform_to_pred(lists, n=2, stop_words=get_popular_stopwords('lines.txt', 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-preservation",
   "metadata": {},
   "source": [
    "# Clustering with Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "touched-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def iterative_lev(a, b):\n",
    "    T = [[0 for _ in range(len(b) + 1)] for _ in range(len(a) + 1)]\n",
    "    # Fill first iter\n",
    "    for x in range(len(b) + 1):\n",
    "        T[0][x] = x\n",
    "    for x in range(len(a) + 1):\n",
    "        T[x][0] = x\n",
    "    # Do the rest\n",
    "    for x in range(1, len(a) + 1):\n",
    "        for y in range(1, len(b) + 1):\n",
    "            T[x][y] = min(\n",
    "                T[x-1][y] + 1,\n",
    "                T[x][y-1] + 1,\n",
    "                T[x-1][y-1] + int(a[x-1] != b[y-1])\n",
    "            )\n",
    "    return T, T[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "seven-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lev(text):\n",
    "    def lev_wrapper(x, y, text):\n",
    "        return iterative_lev(text[int(x[0])], text[int(y[0])])[1] / max(len(text[int(x[0])]), len(text[int(y[0])]))\n",
    "    return lambda x, y: lev_wrapper(x, y, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-wagon",
   "metadata": {},
   "source": [
    "For input >= 1K lines DBSCAN worked for over 30 minutes without solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "modular-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lines.txt', 'r') as f:\n",
    "    raw_text = f.read()\n",
    "text = [x for x in raw_text.split(' ') if len(x) > 0]\n",
    "text = text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "experienced-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(len(text)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "coupled-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.1,\n",
    "    min_samples=1,\n",
    "    metric=get_lev(text),\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "particular-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "disturbed-mixture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358/358 [00:04<00:00, 73.68it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.375"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(X, pred, metric=get_lev(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "built-monte",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358/358 [00:04<00:00, 73.69it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10787333502039155"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(X, pred, metric=get_lev(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "awful-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.1,\n",
    "    min_samples=3,\n",
    "    metric=get_lev(text),\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "yellow-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "statewide-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 158.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5714285714285714"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(X, pred, metric=get_lev(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "documented-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 168.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12362258595918726"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(X, pred, metric=get_lev(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "planned-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = from_file('clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "married-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clusters = [item for member in example for item in member]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cleared-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pred = np.array([i for i, member in enumerate(example) for _ in member])[:500].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "foreign-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.arange(len(text_clusters)).reshape(-1, 1)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "careful-norwegian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [01:55<00:00,  1.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05718954248366013"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(X2, text_pred, metric=get_lev(text_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "lonely-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [01:52<00:00,  1.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.486728231889996"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(X2, text_pred, metric=get_lev(text_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-analysis",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "falling-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = get_popular_stopwords('lines.txt', 20)\n",
    "with open('lines.txt', 'r') as f:\n",
    "    raw_text = f.read()\n",
    "text = [\"\".join(filter(lambda y: y not in stop, x.split(\" \"))) for x in raw_text.split(' ')]\n",
    "text = [x for x in text if len(x) > 0]\n",
    "text = text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dangerous-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.1,\n",
    "    min_samples=3,\n",
    "    metric=get_lev(text),\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cutting-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(len(text)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "animated-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "clear-copying",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 161.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.095238095238095"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(X, pred, metric=get_lev(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "palestinian-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 177.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11837131159281895"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(X, pred, metric=get_lev(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-stage",
   "metadata": {},
   "source": [
    "## Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "upper-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize_file(\n",
    "    'lines.txt', 2)\n",
    "dense_stop = vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "humanitarian-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.1,\n",
    "    min_samples=3,\n",
    "    metric=sk_dense_euc_dist,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "muslim-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(dense_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "retired-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 163.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0066062474614956"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(dense_stop, pred, sk_dense_euc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "contrary-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 207.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.151374657977153"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(dense_stop, pred, sk_dense_euc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "wrapped-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize_file(\n",
    "    'lines.txt', 2,\n",
    "    stop_words=get_popular_stopwords('lines.txt', 20))\n",
    "dense_stop = vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "peripheral-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(\n",
    "    eps=0.1,\n",
    "    min_samples=3,\n",
    "    metric=sk_dense_euc_dist,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "supreme-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.fit_predict(dense_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "private-blast",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-9f70c32b6d07>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"sk_dense_euc_dist\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<function norm at 0x7f8078123af0>) found for signature:\n",
      " \n",
      " >>> norm(array(int64, 1d, C))\n",
      " \n",
      "There are 2 candidate implementations:\n",
      "\u001b[1m      - Of which 2 did not match due to:\n",
      "      Overload in function 'norm_impl': File: numba/np/linalg.py: Line 2352.\n",
      "        With argument(s): '(array(int64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: \u001b[1mnp.linalg.norm() only supported on float and complex arrays.\u001b[0m\u001b[0m\n",
      "  raised from /home/thmtt/.local/lib/python3.9/site-packages/numba/np/linalg.py:897\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<function norm at 0x7f8078123af0>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at <ipython-input-11-9f70c32b6d07> (3)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-11-9f70c32b6d07>\", line 3:\u001b[0m\n",
      "\u001b[1mdef sk_dense_euc_dist(a, b):\n",
      "\u001b[1m    a_norm = a / dense_norm(a)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "/home/thmtt/.local/lib/python3.9/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"sk_dense_euc_dist\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-11-9f70c32b6d07>\", line 2:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef sk_dense_euc_dist(a, b):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/thmtt/.local/lib/python3.9/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-11-9f70c32b6d07>\", line 2:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef sk_dense_euc_dist(a, b):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "100%|██████████| 18/18 [00:00<00:00, 232.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.089341235412819"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(dense_stop, pred, sk_dense_euc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "awful-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 208.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1397089972674707"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(dense_stop, pred, sk_dense_euc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "universal-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = from_file('clusters.txt')\n",
    "c_content, c_pred = transform_to_pred(lists, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "important-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3355/3355 [18:49<00:00,  2.97it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05053991084215299"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn_index(c_content.todense() , c_pred, sk_dense_euc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "built-wireless",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3355/3355 [19:17<00:00,  2.90it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7749570862371746"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index(c_content.todense(), c_pred, sk_dense_euc_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-delivery",
   "metadata": {},
   "source": [
    "# Comparison of clustering predictions\n",
    "\n",
    "Niektóre komórki z wynikami w tabeli nie są bezpośrednio dostępne w notatniku z powodu ponownego wykorzystywania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-district",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th> Metric (used stopwords)</th>\n",
    "        <th> Minimum cluster size </th>\n",
    "        <th> Dunn index (higher = better) </th>\n",
    "        <th> Dunn index for clusters.txt </th>\n",
    "        <th> DB index (lower = better) </th>\n",
    "        <th> DB index for clusters.txt </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> Cosine (none) </td>\n",
    "        <td> 3 </td>\n",
    "        <td> 0.32064 </td>\n",
    "        <td> 0.00255 </td>\n",
    "        <td> 1.64100 </td>\n",
    "        <td> 3.21566 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Cosine (none) </td>\n",
    "        <td> 1 </td>\n",
    "        <td> 0.31986 </td>\n",
    "        <td> 0.00255 </td>\n",
    "        <td> 1.13884 </td>\n",
    "        <td> 3.21566 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Cosine (20 most common words) </td>\n",
    "        <td> 3 </td>\n",
    "        <td> 0.29035 </td>\n",
    "        <td> 0.00255 </td>\n",
    "        <td> 1.61483 </td>\n",
    "        <td> 3.21566 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Cosine (20 most common words) </td>\n",
    "        <td> 1 </td>\n",
    "        <td> 0.26661 </td>\n",
    "        <td> 0.00255 </td>\n",
    "        <td> 0.65242 </td>\n",
    "        <td> 3.21566 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Levenshtein (none) </td>\n",
    "        <td> 3 </td>\n",
    "        <td> 1.57142 </td>\n",
    "        <td> 0.05718 </td>\n",
    "        <td> 0.12362 </td>\n",
    "        <td> 4.48672 </td>\n",
    "    <tr>\n",
    "        <td> Levenshtein (none) </td>\n",
    "        <td> 1 </td>\n",
    "        <td> 1.37500 </td>\n",
    "        <td> 0.05718 </td>\n",
    "        <td> 0.10787 </td>\n",
    "        <td> 4.48672 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Levenshtein (20 most common words) </td>\n",
    "        <td> 3 </td>\n",
    "        <td> 2.09523 </td>\n",
    "        <td> 0.05718 </td>\n",
    "        <td> 0.11837 </td>\n",
    "        <td> 4.48672 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Euclidean (none) </td>\n",
    "        <td> 3 </td>\n",
    "        <td> 1.00660 </td>\n",
    "        <td> 0.05053 </td>\n",
    "        <td> 0.15137 </td>\n",
    "        <td> 1.77495 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Euclidean (20 most common words) </td>\n",
    "        <td> 3 </td>\n",
    "        <td> 1.08934 </td>\n",
    "        <td> 0.05053 </td>\n",
    "        <td> 0.13970 </td>\n",
    "        <td> 1.77495 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-crime",
   "metadata": {},
   "source": [
    "Dla każdej klasteryzacji dla obu sposobów oceny klasteryzacji wyniki są lepsze od klasteryzacji zaproponowanej w pliku `clusters.txt`. Niektóre wyżej wymienione klasteryzacje nie klasyfikowały wszystkich adresów (w przypadku n_samples=3 odrzucane były klastry o rozmiarze 1 lub 2 traktowane jako szum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-amazon",
   "metadata": {},
   "source": [
    "# Ex 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-burner",
   "metadata": {},
   "source": [
    "Dla podanego pliku teskstowe `lines.txt` poza stoplistą w celu polepszenia klasteryzacji można by było wypróbować:\n",
    "- _Stemming_ tekstu\n",
    "- Wykorzystanie _Inverse Document Frequency_ (IDF)\n",
    "- Oczyszczanie tekstu (np. usunięcie wielokrotnych spacji)\n",
    "- Usunięcie interpunkcji\n",
    "- Sprawdzenie innych parametrów DBSCAN\n",
    "- Sprawdzenie innych technik klasteryzacji"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
